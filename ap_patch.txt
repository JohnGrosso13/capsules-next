*** Begin Patch
*** Update File: src/lib/ai/prompter/core.ts
import { fetchOpenAI, hasOpenAIApiKey } from "@/adapters/ai/openai/server";
import { serverEnv } from "@/lib/env/server";

export type Json = string | number | boolean | null | Json[] | { [key: string]: Json };

export type ChatMessage = Record<string, unknown>;

export type JsonSchema = { name: string; schema: Record<string, unknown> };

export class AIConfigError extends Error {
  constructor(message: string) {
    super(message);

    this.name = "AIConfigError";
  }
}

export function requireOpenAIKey() {
  if (!hasOpenAIApiKey()) {
    throw new AIConfigError(
      "OpenAI API key is not configured. Set OPENAI_API_KEY in the environment.",
    );
  }
}

export function extractJSON<T = Record<string, unknown>>(maybeJSONString: unknown): T | null {
  if (maybeJSONString && typeof maybeJSONString === "object") {
    return maybeJSONString as T;
  }

  const text = String(maybeJSONString ?? "");

  try {
    return JSON.parse(text) as T;
  } catch {
    // continue
  }

  try {
    const fenced = text.replace(/^\s*```(?:json)?\s*/i, "").replace(/\s*```\s*$/i, "");

    return JSON.parse(fenced) as T;
  } catch {
    // continue
  }

  try {
    const start = text.indexOf("{");

    const end = text.lastIndexOf("}");

    if (start >= 0 && end > start) {
      return JSON.parse(text.slice(start, end + 1)) as T;
    }
  } catch {
    // ignore incomplete fragments
  }

  return null;
}

function approximateSize(messages: ChatMessage[]): number {
  try {
    return JSON.stringify(messages).length;
  } catch {
    return 0;
  }
}

function coerceString(value: unknown): string {
  return typeof value === "string" ? value : String(value ?? "");
}

function compactContent(content: string, limit: number): string {
  if (content.length <= limit) return content;
  return `${content.slice(0, Math.max(0, limit - 3))}...`;
}

function stripNoisyLines(text: string): string {
  const lines = text.split("\n");
  const filtered = lines.filter((line) => {
    const lower = line.toLowerCase();
    if (lower.startsWith("media: ")) return false;
    if (lower.startsWith("attachments referenced:")) return false;
    return true;
  });
  return filtered.join("\n");
}

function compactMessagesForBudget(messages: ChatMessage[], budget = 50000): ChatMessage[] {
  let result = messages.map((m) => ({ ...m }));
  const size = (() => { try { return JSON.stringify(result).length; } catch { return 0; } })();
  if (size <= budget) return result;

  // 1) Keep latest assistant turn only
  const lastAssistantIndex = [...result].reverse().findIndex((m) => (m as any).role === "assistant");
  if (lastAssistantIndex >= 0) {
    const absoluteIndex = result.length - 1 - lastAssistantIndex;
    result = result.filter((m, i) => (m as any).role !== "assistant" || i === absoluteIndex);
  }
  if ((() => { try { return JSON.stringify(result).length; } catch { return 0; } })() <= budget) return result;

  // 2) Strip noisy lines and hard-truncate long system/context blocks
  result = result.map((m) => {
    const role = (m as any).role;
    const content = coerceString((m as any).content);
    let cleaned = stripNoisyLines(content);
    if (role === "system") {
      const isUserCard = /\bUser profile:/i.test(cleaned);
      const isContext = /\bContext memories to ground your response:/i.test(cleaned);
      cleaned = compactContent(cleaned, isContext ? 4000 : isUserCard ? 1200 : 3000);
    } else {
      cleaned = compactContent(cleaned, 4000);
    }
    return { ...m, content: cleaned } as ChatMessage;
  });
  if ((() => { try { return JSON.stringify(result).length; } catch { return 0; } })() <= budget) return result;

  // 3) Keep only: first system message, optional context/system message, last user, last assistant
  const systems = result.filter((m) => (m as any).role === "system");
  const firstSystem = systems[0] ? [systems[0]] : [];
  const extraSystem = systems[1] ? [systems[1]] : [];
  const lastUserIndex = [...result].reverse().findIndex((m) => (m as any).role === "user");
  const lastUser = lastUserIndex >= 0 ? [result[result.length - 1 - lastUserIndex]] : [];
  const lastAssistantIdx = [...result].reverse().findIndex((m) => (m as any).role === "assistant");
  const lastAssistant = lastAssistantIdx >= 0 ? [result[result.length - 1 - lastAssistantIdx]] : [];
  result = [...firstSystem, ...extraSystem, ...lastUser, ...lastAssistant];
  if ((() => { try { return JSON.stringify(result).length; } catch { return 0; } })() <= budget) return result;

  // 4) As a final fallback, keep only first system and last user
  result = [...firstSystem, ...lastUser];
  return result;
}
export async function callOpenAIChat(
  messages: ChatMessage[],

  schema: JsonSchema | null,

  options: { temperature?: number } = {},
): Promise<{ content: string; raw: Json }> {
  requireOpenAIKey();

  const retryDelaysMs = [0, 800, 1600];
  const temperature = options.temperature ?? 0.7;

  const payload: Record<string, unknown> = {
    model: serverEnv.OPENAI_MODEL,

    messages,

    temperature,
  };

  if (schema) {
    payload.response_format = { type: "json_schema", json_schema: schema };
  } else {
    payload.response_format = { type: "json_object" };
  }

  async function postWithRetries(body: Record<string, unknown>): Promise<{
    response: Response;
    json: Json;
  }> {
    let lastResponse: Response | null = null;
    let lastJson: Json = {};

    for (const delay of retryDelaysMs) {
      if (delay > 0) {
        await new Promise((resolve) => setTimeout(resolve, delay));
      }

      const response = await fetchOpenAI("/chat/completions", {
        method: "POST",

        headers: {
          "Content-Type": "application/json",
        },

        body: JSON.stringify(body),
      });

      const json = (await response.json().catch(() => ({}))) as Json;
      if (response.ok) {
        return { response, json };
      }

      lastResponse = response;
      lastJson = json;

      if (response.status !== 429 && response.status < 500) {
        break;
      }
    }

    return { response: lastResponse as Response, json: lastJson };
  }

  let { response, json } = await postWithRetries(payload);

  if (!response.ok) {
    const fallbackBody = { model: serverEnv.OPENAI_MODEL, messages, temperature };

    ({ response, json } = await postWithRetries(fallbackBody));

    if (!response.ok) {
      const error = new Error(`OpenAI chat error: ${response.status}`);

      (error as Error & { meta?: Json }).meta = json;

      throw error;
    }
  }

  const choices = (json as Record<string, unknown>).choices;

  const content = Array.isArray(choices)
    ? (choices[0] as Record<string, unknown>)?.message &&
      ((choices[0] as Record<string, unknown>).message as Record<string, unknown>)?.content
    : null;

  if (!content || typeof content !== "string") {
    throw new Error("OpenAI chat returned empty content.");
  }

  return { content, raw: json };
}

*** End Patch

